IRCTC Real-Time Data Streaming Pipeline (GCP)
ğŸ“Œ Project Overview

This project demonstrates an end-to-end real-time data engineering pipeline built on Google Cloud Platform (GCP).
Mock IRCTC-style user data is generated using Python, published to Pub/Sub, processed using Dataflow (Apache Beam), and finally streamed into BigQuery for analytics.

All resources were created, tested, and cleanly deleted after execution to ensure cost and security best practices.

ğŸ—ï¸ Architecture

Python Publisher
â†“
Google Cloud Pub/Sub
â†“
Google Cloud Dataflow (Streaming)
â†“
Google BigQuery

ğŸ§° Tech Stack

Python

Google Cloud Pub/Sub

Google Cloud Dataflow (Apache Beam)

Google BigQuery

Google Cloud Storage (staging & temp)

âš™ï¸ Data Pipeline Description

1.Python script generates mock IRCTC user data in JSON format

2.Messages are published to a Pub/Sub topic

3.Dataflow streaming job reads messages from Pub/Sub

4.Messages are transformed into BigQuery-compatible rows

5.Valid records are written into BigQuery table

6.Failed records (if any) are routed to error handling steps

ğŸ§¾ BigQuery Schema (Streaming Table)

row_key STRING,
name STRING,
age INT64,
email STRING,
join_date DATE,
last_login DATETIME,
loyalty_points INT64,
account_balance FLOAT64,
is_active BOOL,
inserted_at DATETIME,
updated_at DATETIME,
loyalty_status STRING,
account_age_days INT64

ğŸ“¸ Execution Proof
ğŸ“Š BigQuery â€“ Query Results

Real-time data successfully streamed and queried from BigQuery.

ğŸ”„ Dataflow â€“ Streaming Job Graph (View 1)

End-to-end pipeline showing Pub/Sub ingestion and BigQuery sink.

ğŸ”„ Dataflow â€“ Streaming Job Graph (View 2)

Expanded view highlighting successful and failed record handling stages.

ğŸ“œ Dataflow â€“ Job Logs

Pipeline execution logs showing successful job initialization and runtime behavior.

ğŸ§  Dataflow â€“ Worker Logs

Worker-level logs indicating healthy execution, low latency, and zero data lag.

ğŸ§ª Observations & Metrics

Streaming latency: < 1 second

Data lag: 0 seconds

Successful writes to BigQuery

Graceful shutdown using Drain

No data loss during termination

ğŸ” Security & Cost Management

Service account keys revoked after use

Environment variables cleared

All cloud resources deleted post-testing

No active Pub/Sub, Dataflow, BigQuery, or GCS resources

Zero residual billing

ğŸ¯ Learning Outcomes

Designed a real-time streaming architecture on GCP

Understood Pub/Sub topic vs subscription behavior

Implemented schema-driven BigQuery ingestion

Managed Dataflow job lifecycle (run, monitor, drain)

Applied cloud security and cost best practices

ğŸš€ How to Run (High-Level)

Create Pub/Sub topic

Create BigQuery dataset & table

Create BigQuery subscription or Dataflow job

Run Python publisher script

Monitor Dataflow and query BigQuery

ğŸ“Œ Project Status

âœ” Completed
âœ” Tested
âœ” Resources cleaned up

Built a real-time data streaming pipeline using Google Cloud Pub/Sub, Dataflow, and BigQuery; validated ingestion through live Dataflow job graphs, worker logs, and BigQuery query results while following security and cost best practices.

ğŸ“‚ Repository Structure
.
â”œâ”€â”€ publisher.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ screenshots/
â”œâ”€â”€ bigquery-query-result.png
â”œâ”€â”€ dataflow-job-graph-1.png
â”œâ”€â”€ dataflow-job-graph-2.png
â”œâ”€â”€ dataflow-job-logs.png
â””â”€â”€ dataflow-worker-logs.png

ğŸ™Œ Final Note

This project focuses on real-world data engineering practices, including streaming ingestion, schema enforcement, monitoring, and responsible cloud cleanup.
